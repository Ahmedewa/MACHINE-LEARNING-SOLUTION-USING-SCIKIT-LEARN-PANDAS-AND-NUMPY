           BACKUP STRATEGIES


## **1. Backup Strategy**

### **1.1 Training Data Backup**
Your **training data** includes raw datasets and feature-engineered data. Backing up these files ensures you can retrain your models in case of data loss.

#### **Code: Backup Training Data to Cloud Storage**

1. **Using AWS S3 for Backup**
   ```python
   import boto3
   import os
   from datetime import datetime

   # AWS S3 Configuration
   s3 = boto3.client('s3')
   bucket_name = "ml-app-backups"
   data_dir = "./data/training_data/"
   timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

   def backup_training_data():
       for file in os.listdir(data_dir):
           if file.endswith(".csv") or file.endswith(".parquet"):
               file_path = os.path.join(data_dir, file)
               backup_key = f"backups/{timestamp}/{file}"
               s3.upload_file(file_path, bucket_name, backup_key)
               print(f"Uploaded: {file} to {backup_key}")
   backup_training_data()
   ```

2. **Using Google Cloud Storage (GCS)**
   ```python
   from google.cloud import storage
   import os

   # GCS Configuration
   client = storage.Client()
   bucket = client.bucket("ml-app-backups")
   data_dir = "./data/training_data/"

   def backup_to_gcs():
       for file in os.listdir(data_dir):
           if file.endswith(".csv") or file.endswith(".parquet"):
               blob = bucket.blob(f"training_data/{file}")
               blob.upload_from_filename(f"{data_dir}/{file}")
               print(f"Uploaded: {file}")
   backup_to_gcs()
   ```

#### **Best Practices**
- Use **versioning** in your storage buckets to keep track of changes.
- Compress large datasets to save storage and reduce upload time:
  ```bash
  tar -czvf training_data.tar.gz ./data/training_data/
  ```

---

### **1.2 Backup Trained Models**
Serialized models (e.g., `.pkl`, `.h5`) must be backed up to ensure you can redeploy without retraining.

#### **Code: Backup Models**
1. **AWS S3**
   ```python
   def backup_model():
       model_path = "./models/model.pkl"
       backup_key = f"backups/models/{timestamp}-model.pkl"
       s3.upload_file(model_path, bucket_name, backup_key)
       print(f"Uploaded model to {backup_key}")
   backup_model()
   ```

2. **Google Cloud**
   ```python
   def backup_model_to_gcs():
       blob = bucket.blob(f"models/{timestamp}-model.pkl")
       blob.upload_from_filename("./models/model.pkl")
       print("Model backed up to GCS.")
   backup_model_to_gcs()
   ```

---

### **1.3 Backup Configuration Files**
Configuration files like `config.json` store hyperparameters, pipeline settings, and environment variables.

#### **Code: Backup Configuration Files**
```python
def backup_config():
    config_path = "./config/config.json"
    backup_key = f"backups/config/{timestamp}-config.json"
    s3.upload_file(config_path, bucket_name, backup_key)
    print(f"Uploaded config file to {backup_key}")
backup_config()
```

---

### **1.4 Backup Application Code**
Application code includes scripts, APIs, and deployment files.

#### **Code: Automate Code Backup**
1. **Push Code to a Git Repository**
   ```bash
   git init
   git remote add origin https://github.com/your-repo/ml-app.git
   git add .
   git commit -m "Backup ML app source code"
   git push origin main
   ```

2. **Backup to Cloud**
   ```python
   def backup_code():
       os.system(f"zip -r ml-app-backup-{timestamp}.zip ./app/")
       s3.upload_file(f"./ml-app-backup-{timestamp}.zip", bucket_name, f"backups/code/{timestamp}-ml-app.zip")
       print("Code backed up successfully.")
   backup_code()
   ```

---
