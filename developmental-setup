
                         DEVELOPMENTAL SETUP

## **1. Developmental Setup for an ML App**

Setting up a robust developmental environment ensures efficiency, scalability, and collaboration throughout the ML lifecycle. Below are the **components**, **tools**, and **best practices** for a complete setup.

---

### **1.1 Key Components of the Developmental Setup**

1. **Environment Management**:
   - Use **virtual environments** to isolate dependencies.
   - Tools: `venv`, `conda`, or Docker.

2. **Code Versioning**:
   - Manage code using **Git** and a **Git hosting service** (e.g., GitHub, GitLab).

3. **Data Management**:
   - Version datasets using **DVC** (Data Version Control).
   - Store training data in **cloud buckets** (AWS S3, GCS).

4. **Model Management**:
   - Track model experiments using **MLflow** or **Weights & Biases**.

5. **Testing**:
   - Unit testing for data pipelines and ML models.
   - Use `pytest` with frameworks like `unittest` or `Great Expectations`.

6. **Code Quality**:
   - Enforce linting and formatting using `pylint` or `black`.

7. **Continuous Integration/Continuous Deployment (CI/CD)**:
   - Automate testing, model deployments, and pipelines using **GitHub Actions**, **Jenkins**, or **GitLab CI/CD**.

8. **Containerization**:
   - Use **Docker** to package your app for portability.

9. **Infrastructure as Code (IaC)**:
   - Provision cloud resources with **Terraform** or **AWS CloudFormation**.

---

### **1.2 Setting Up Each Component**

#### **Step 1: Environment Setup**
1. **Create a Virtual Environment**
   ```bash
   python3 -m venv ml-env
   source ml-env/bin/activate
   pip install --upgrade pip
   ```

2. **Install Core ML Libraries**
   ```bash
   pip install numpy pandas scikit-learn matplotlib seaborn tensorflow
   ```

3. **Add a `requirements.txt` File**
   ```plaintext
   numpy==1.21.6
   pandas==1.3.5
   scikit-learn==1.0.2
   tensorflow==2.9.1
   ```
   Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. **Set Up Jupyter Notebook**
   ```bash
   pip install notebook
   jupyter notebook
   ```

---

#### **Step 2: Data Management**
1. **Initialize DVC**
   ```bash
   pip install dvc
   dvc init
   dvc remote add -d myremote s3://your-bucket-name
   ```

2. **Track Datasets**
   ```bash
   dvc add data/training_data.csv
   git add data/.gitignore data/training_data.csv.dvc
   git commit -m "Track training data with DVC"
   dvc push
   ```

---

#### **Step 3: Model Experimentation**
1. **Install MLflow**
   ```bash
   pip install mlflow
   ```

2. **Track Experiments**
   ```python
   import mlflow
   from sklearn.ensemble import RandomForestClassifier

   mlflow.start_run()
   model = RandomForestClassifier(n_estimators=100)
   mlflow.log_param("n_estimators", 100)
   mlflow.log_metric("accuracy", 0.95)
   mlflow.end_run()
   ```

---

#### **Step 4: Testing**
1. **Install `pytest`**
   ```bash
   pip install pytest
   ```

2. **Write a Unit Test**
   Create `test_model.py`:
   ```python
   import pytest
   from sklearn.ensemble import RandomForestClassifier

   def test_model_training():
       model = RandomForestClassifier(n_estimators=10)
       assert model is not None
   ```

3. **Run Tests**
   ```bash
   pytest test_model.py
   ```

---

#### **Step 5: CI/CD**
1. **GitHub Actions Workflow**
   Create a `.github/workflows/ci.yml` file:
   ```yaml
   name: CI Pipeline

   on:
     push:
       branches:
         - main

   jobs:
     test:
       runs-on: ubuntu-latest
       steps:
         - name: Checkout Code
           uses: actions/checkout@v3

         - name: Set Up Python
           uses: actions/setup-python@v4
           with:
             python-version: 3.9

         - name: Install Dependencies
           run: pip install -r requirements.txt

         - name: Run Tests
           run: pytest
   ```

2. **Automate Model Deployment**
   Use **Docker** to containerize the app and deploy to **AWS ECS** or **Google Kubernetes Engine (GKE)**.

---

#### **Step 6: Linting & Formatting**
1. **Install `black`**
   ```bash
   pip install black
   ```

2. **Run Linter**
   ```bash
   black .
   ```

---

#### **Step 7: Containerization**
1. **Create a `Dockerfile`**
   ```dockerfile
   FROM python:3.9-slim

   WORKDIR /app
   COPY . /app
   RUN pip install -r requirements.txt

   CMD ["python", "app.py"]
   ```

2. **Build and Run**
   ```bash
   docker build -t ml-app .
   docker run -p 5000:5000 ml-app
   ```

---

### **1.3 Best Practices**
1. **Follow a Modular Code Structure**:
   - Example:
     ```
     /app
       ├── data/
       ├── models/
       ├── src/
       │   ├── pipeline.py
       │   ├── train.py
       │   └── predict.py
       ├── tests/
       └── config/
           └── config.json
     ```

2. **Enable GPU Support for Training**:
   - Use a **Docker image** with GPU support:
     ```dockerfile
     FROM tensorflow/tensorflow:latest-gpu
     ```

3. **Use Pre-commit Hooks**:
   - Install `pre-commit`:
     ```bash
     pip install pre-commit
     pre-commit install
     ```

---

## **2. Single Technical Diagram**

Below is a **single technical diagram** to visualize the **architecture of your ML app’s development and deployment workflow**:

```plaintext
                                    +----------------------------------+
                                    |      Version Control (Git)       |
                                    +----------------------------------+
                                                |
                                                v
    +------------------------------+   +------------------------------+
    |      Development Setup       |   |       Data Management        |
    |  - Virtual Environment       |   |  - Dataset Versioning (DVC)  |
    |  - Jupyter Notebooks         |   |  - Cloud Storage (AWS S3)    |
    +------------------------------+   +------------------------------+
                   |                                     |
                   v                                     v
    +------------------------------------------------------------+
    |                  Model Experimentation                     |
    |  - MLflow for Experiment Tracking                          |
    |  - Hyperparameter Tuning                                   |
    +------------------------------------------------------------+
                             |
                             v
    +------------------------------------------------------------+
    |               CI/CD (GitHub Actions, Jenkins)              |
    |  - Automated Testing                                        |
    |  - Model Deployment (Docker, Kubernetes)                   |
    +------------------------------------------------------------+
                             |
                             v
    +------------------------------------------------------------+
    |                   Monitoring and Logging                   |
    |  - Prometheus + Grafana                                    |
    |  - CloudWatch/Azure Monitor                                |
    +------------------------------------------------------------+
```

